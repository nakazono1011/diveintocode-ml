{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sprint15",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPV4qjscJhs/niEG1web2W/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVNbvf5yyjYV",
        "colab_type": "text"
      },
      "source": [
        "# (1) 物体検出の分野にはどういった手法が存在したか。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-_n_lTnFLXf",
        "colab_type": "text"
      },
      "source": [
        "【解】  \n",
        "* SPPNet  (参考論文：Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition)\n",
        "  * 入力画像のサイズに依らず、任意のサイズ/スケールの画像の認識精度を上げる手法。空間ピラミッドプーリング層を用いることにより、矩形の物体についても認識できるため物体検出の技術にも用いられる  \n",
        "  \n",
        "* R-CNN  (参考論文：Contextual Action Recognition with R*CNN)\n",
        "  * 画像中の人の行動や物体の属性を認識する際に、それ自体のローカライズされた情報だけでなく、周りの情報も特徴として盛り込むことで、画像中の文脈や背景を加味した画像認識を行う手法\n",
        "\n",
        "【論文中の根拠】\n",
        "* Abstract：\n",
        "  * (2~3)Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region\n",
        "  proposal computation as a bottleneck"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HAlEbjfyjgb",
        "colab_type": "text"
      },
      "source": [
        "# (2) Fasterとあるが、どういった仕組みで高速化したのか。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ58Vp5VDnt3",
        "colab_type": "text"
      },
      "source": [
        "【解】  \n",
        "\n",
        "* 従来の手法は，領域提案に費やされた時間が最大のボトルネックであったため，Selective Searchによる領域提案の手法をRPN（Region Proposal Network）という小さい畳み込みネットワークに入れ替えて、RPN上で領域提案と物体検出を行うことによりリアルタイムに近い計算速度を達成した\n",
        "\n",
        "【論文中の根拠】\n",
        "* Introduction:\n",
        "  * (page1) The latest incarnation, Fast R-CNN [2], achieves near real-time rates using very deep networks [3], when ignoring the time spent on region proposals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Crynmh5Byq_d",
        "colab_type": "text"
      },
      "source": [
        "# (3) One-Stageの手法とTwo-Stageの手法はどう違うのか。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLEPA5mAD3V8",
        "colab_type": "text"
      },
      "source": [
        "【解】  \n",
        "* ニューラルネットワークを用いた物体検出手法は、検出を１段階で行うか２段階で行うかによって、それぞれOne-Stage DetectionとTwo-Stage Detectionに分類される。 One-Stage DetectionはBoundingBox生成とクラス予測を一度に行う。One-Stage Detectionは構成がシンプルになり、学習や推論の高速化が見込める。検出精度はTwo-Stage Detectionに劣る傾向がある。 Two-Stage Detectionでは、まず1段階目でBoundingBoxの候補を抽出する。そして2段階目でクラス予測とBoundingBoxの修正を行い、最終的な予測結果を生成する。\n",
        "\n",
        "【論文中の根拠】\n",
        "\n",
        "* (page10)The OverFeat paper [9] proposes a detection method that uses regressors and classifiers on sliding windows over convolutional feature maps. OverFeat is a one-stage, class-specific detection pipeline, and ours is a two-stage cascade consisting of class-agnostic proposals and class-specific detections.\n",
        "\n",
        "【参考論文】\n",
        "* OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwQr9OOoyrD2",
        "colab_type": "text"
      },
      "source": [
        "# (4) RPNとは何か。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDbisB5eEXrQ",
        "colab_type": "text"
      },
      "source": [
        "【解】\n",
        "* RPNは「ある画像のどこに物体が写っているか」＝「物体が写っている場所と、その矩形の形」を検出できる機械学習モデル\n",
        "\n",
        "【論文中の根拠】\n",
        "* Introduction\n",
        "  * (page1)The RPN is thus a kind of fully convolutional network (FCN) [7] and can be trained end-toend specifically for the task for generating detection proposals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVpeeoRlyrMG",
        "colab_type": "text"
      },
      "source": [
        "# (5) RoIプーリングとは何か。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WzAEGF8Kr3M",
        "colab_type": "text"
      },
      "source": [
        "【解】\n",
        "* RoI Poolingは分類を行う層への入力を固定次元にする役割がある。物体候補の領域に対応する特徴マップをRoI Poolingにより切り出すことで特徴抽出を共通化し高速化する。画像のサイズが異なった場合においても、次元を固定化する手法\n",
        "\n",
        "【論文中の根拠】\n",
        "\n",
        "* (page6) The RoI pooling layer [2] in Fast R-CNN accepts the convolutional features and also the predicted bounding boxes as input, so a theoretically valid backpropagation solver should also involve gradients w.r.t. the box coordinates. These gradients are ignored in the above approximate joint training. In a non-approximate joint training solution, we need an RoI pooling layer that is differentiable w.r.t. the box coordinates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_paX8550yrKI",
        "colab_type": "text"
      },
      "source": [
        "# (6) Anchorのサイズはどうするのが適切か。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7EGJ0ScK-b2",
        "colab_type": "text"
      },
      "source": [
        "【解】\n",
        "* Anchorは物体の領域を示している矩形で、画像内に一定間隔で生成されLoss Functionで利用される。 論文の実験では以下 3 x 3 = 9 パターンが使われている。\n",
        "\n",
        "  * スケール: $128^2$, $256^2$, $512^2$\n",
        "  * アスペクト比: 1:1, 1:2, 2:1\n",
        "  * Anchorはドメイン次第でチューニングするのが良いものと考えられる。\n",
        "\n",
        "【論文中の根拠】\n",
        "* (page6) For anchors, we use 3 scales with box areas of $128^2$, $256^2$,and $512^2$ pixels, and 3 aspect ratios of 1:1, 1:2, and 2:1. These hyper-parameters are not carefully chosen for a particular dataset, and we provide ablation experiments on their effects in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIXrYsnwyrH8",
        "colab_type": "text"
      },
      "source": [
        "# (7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LO_5_aGExIv",
        "colab_type": "text"
      },
      "source": [
        "【解】\n",
        "* 試用したデータセット： MS COCO dataset\n",
        "* 先行研究との比較結果\n",
        "  <img src=\"https://github.com/obata01/diveintocode-ml/raw/1c3d427fd5fc6d3ece9b9c2dc6aea05d016e5867/kadai/Sprint15/kadai/img/result.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fBCTEuryrCT",
        "colab_type": "text"
      },
      "source": [
        "# (8) （アドバンス課題）Faster R-CNNよりも新しい物体検出の論文では、Faster R-CNNがどう引用されているか。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut-zQRPDTE7i",
        "colab_type": "text"
      },
      "source": [
        "解答なし"
      ]
    }
  ]
}