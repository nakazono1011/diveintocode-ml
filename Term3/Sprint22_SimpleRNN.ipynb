{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sprint22_SimpleRNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOjb2+ICnogWpWdlMCU4/xV"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu2qZoU_8m_x"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr7hPmRL8r5R"
      },
      "source": [
        "# このSprintについて\n",
        "\n",
        "## Sprintの目的\n",
        "* スクラッチを通してリカレントニューラルネットワークの基礎を理解する\n",
        "\n",
        "## どのように学ぶか\n",
        "* スクラッチでリカレントニューラルネットワークの実装を行います。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4faKbdxh8r2z"
      },
      "source": [
        "# リカレントニューラルネットワークスクラッチ\n",
        "\n",
        "リカレントニューラルネットワーク（RNN） のクラスをスクラッチで作成していきます。\n",
        "NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
        "\n",
        "フォワードプロパゲーションの実装を必須課題とし、バックプロパゲーションの実装はアドバンス課題とします。\n",
        "\n",
        "クラスの名前はScratchSimpleRNNClassifierとしてください。クラスの構造などは以前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX4zbWzt8ryi"
      },
      "source": [
        "##【問題1】SimpleRNNのフォワードプロパゲーション実装\n",
        "SimpleRNNのクラスSimpleRNNを作成してください。基本構造はFCクラスと同じになります。\n",
        "\n",
        "\n",
        "フォワードプロパゲーションの数式は以下のようになります。ndarrayのshapeがどうなるかを併記しています。\n",
        "\n",
        "\n",
        "バッチサイズをbatch_size、入力の特徴量数をn_features、RNNのノード数をn_nodesとして表記します。活性化関数はtanhとして進めますが、これまでのニューラルネットワーク同様にReLUなどに置き換えられます。\n",
        "\n",
        "\n",
        "初期状態 $h0$ は全て0とすることが多いですが、任意の値を与えることも可能です。\n",
        "\n",
        "\n",
        "上記の処理を系列数n_sequences回繰り返すことになります。RNN全体への入力 \n",
        "x\n",
        " は(batch_size, n_sequences, n_features)のような配列で渡されることになり、そこから各時刻の配列を取り出していきます。\n",
        "\n",
        "\n",
        "分類問題であれば、それぞれの時刻のhに対して全結合層とソフトマックス関数（またはシグモイド関数）を使用します。タスクによっては最後の時刻のhだけを使用することもあります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4OLcrgs9FVt"
      },
      "source": [
        "class SimpleRNN:\n",
        "    def __init__(self, n_nodes, initializer, optimizer, activation, debug=False):\n",
        "        # ノードの最適化・初期化インスタンスを作成\n",
        "        self.optimizer = optimizer\n",
        "        self.initializer = initializer\n",
        "        self.activation  = activation\n",
        "\n",
        "        self.n_nodes = n_nodes\n",
        "        self.debug = debug\n",
        "\n",
        "        # ノードのバイアスを初期化\n",
        "        if self.debug:\n",
        "          self.b = np.array([1, 1, 1, 1])\n",
        "        else:\n",
        "          self.b = self.initializer.B(n_nodes, )\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        X: 入力(batch_size, n_sequences, n_features)\n",
        "\n",
        "        \"\"\"\n",
        "        batch_size  = X.shape[0]\n",
        "        n_sequences = X.shape[1]\n",
        "        n_features  = X.shape[2]\n",
        "\n",
        "        if self.debug:\n",
        "          self.w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100\n",
        "          self.w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100\n",
        "        else:\n",
        "          self.w_x = self.initializer.W(n_features, self.n_nodes)\n",
        "          self.w_h = self.initializer.W(self.n_nodes, self.n_nodes)\n",
        "\n",
        "        h = np.zeros((n_sequences+1, batch_size, self.n_nodes))\n",
        "\n",
        "        for i in range(n_sequences):\n",
        "           a = X[:, i, :] @ self.w_x + h[i, :, :] @ self.w_h + self.b\n",
        "           h[i+1, :, :] = self.activation.forward(a)\n",
        "        print(h)\n",
        "        return h[-1, :, :]\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        実装なし\n",
        "        \"\"\"\n",
        "        \n",
        "        return dZ"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_bcESMk-DCa"
      },
      "source": [
        "初期化クラス"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djMsi8Fa9xwo"
      },
      "source": [
        "class SimpleInitializer:\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "        \n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "    \n",
        "    def B(self, n_nodes2):\n",
        "        B = self.sigma * np.random.randn(n_nodes2, )\n",
        "        return B"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BGPRSZ8-FYY"
      },
      "source": [
        "最適化手法"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po-jOgAb91uf"
      },
      "source": [
        "class SGD:\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    def update(self, layer):\n",
        "        layer.B -= self.lr * layer.dB\n",
        "        layer.W -= self.lr * layer.dW"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN8ENVE5-Ho1"
      },
      "source": [
        "活性化関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHK3GZuN93gG"
      },
      "source": [
        "class Sigmoid:\n",
        "    \"\"\"\n",
        "    Sigmoid関数のクラス\n",
        "    \"\"\"  \n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def forward(self, X):\n",
        "        self.A = X\n",
        "        return 1 / (1 + np.exp(-X))\n",
        "    \n",
        "    def backward(self, X):\n",
        "        return X * (1- self.forward(self.A)) * self.forward(self.A)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx_DBGDM98t4"
      },
      "source": [
        "class Tanh:\n",
        "    \"\"\"\n",
        "    Tanh関数のクラス\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def forward(self, X):\n",
        "        self.A = X\n",
        "        return np.tanh(X)\n",
        "    \n",
        "    def backward(self, X):\n",
        "        return X * (1 - self.forward(self.A)**2)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T5oBE_6989U"
      },
      "source": [
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def forward(self, X):\n",
        "        X = X - np.max(X)\n",
        "        return np.exp(X) / np.sum(np.exp(X), axis=1, keepdims=True)\n",
        "    \n",
        "    def backward(self, X, y):\n",
        "        batch_size = len(X)\n",
        "        delta = 1e-7\n",
        "        \n",
        "        self.loss = -np.sum(y * np.log(X+delta)) / batch_size\n",
        "        return X - y"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTS3oOXg9_qT"
      },
      "source": [
        "class ReLU:\n",
        "    \"\"\"\n",
        "    ReLU関数のクラス\n",
        "    \"\"\"\n",
        "    def forward(self, X):\n",
        "        self.A = X\n",
        "        return np.maximum(0, X)\n",
        "    \n",
        "    def backward(self, X):\n",
        "        return X * (self.A > 0)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmd0TDmz-NcB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsO5DkT1_S0u"
      },
      "source": [
        "## 【問題2】小さな配列でのフォワードプロパゲーションの実験\n",
        "小さな配列でフォワードプロパゲーションを考えてみます。\n",
        "\n",
        "入力x、初期状態h、重みw_xとw_h、バイアスbを次のようにします。\n",
        "\n",
        "ここで配列xの軸はバッチサイズ、系列数、特徴量数の順番です。\n",
        "\n",
        "```python\n",
        "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
        "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
        "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
        "batch_size = x.shape[0] # 1\n",
        "n_sequences = x.shape[1] # 3\n",
        "n_features = x.shape[2] # 2\n",
        "n_nodes = w_x.shape[1] # 4\n",
        "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
        "b = np.array([1, 1, 1, 1]) # (n_nodes,)\n",
        "```\n",
        "\n",
        "フォワードプロパゲーションの出力が次のようになることを作成したコードで確認してください。\n",
        "\n",
        "```python\n",
        "h = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]]) # (batch_size, n_nodes)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD153plIA8mh"
      },
      "source": [
        "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
        "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
        "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
        "batch_size = x.shape[0] # 1\n",
        "n_sequences = x.shape[1] # 3\n",
        "n_features = x.shape[2] # 2\n",
        "n_nodes = w_x.shape[1] # 4\n",
        "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
        "b = np.array([1, 1, 1, 1]) # (n_nodes,)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubudzg_cklvD"
      },
      "source": [
        "フォワード部分のみ計算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M71imCxQA9HU",
        "outputId": "2c11c021-6120-42ab-da09-f004304ad6f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#１系列目\n",
        "activation = Tanh()\n",
        "a1 = x[:, 0, :] @ w_x + h @ w_h + b\n",
        "h1 = activation.forward(a1)\n",
        "#２系列目\n",
        "a2 = x[:, 1, :] @ w_x + h1 @ w_h + b\n",
        "h2 = activation.forward(a2)\n",
        "#３系列目\n",
        "a3 = x[:, 2, :] @ w_x + h2 @ w_h + b\n",
        "h3 = activation.forward(a3)\n",
        "h3\n",
        "\n",
        "print(h1)\n",
        "print(h2)\n",
        "print(h3)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.76188798 0.76213958 0.76239095 0.76255841]]\n",
            "[[0.792209   0.8141834  0.83404912 0.84977719]]\n",
            "[[0.79494228 0.81839002 0.83939649 0.85584174]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo6J7mB9A9pM",
        "outputId": "c1bff1c9-fdd4-4191-cbe7-0c6f5e1c9557",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#初期化クラス\n",
        "initializer = SimpleInitializer(sigma=0.1)\n",
        "#最適化クラス\n",
        "optimizer   = SGD(lr=0.1)\n",
        "#活性化関数クラス\n",
        "activation  = Tanh()\n",
        "\n",
        "#モデル作成\n",
        "rnn = SimpleRNN(n_nodes, initializer=initializer, optimizer=optimizer, activation=activation, debug=True)\n",
        "rnn.forward(x)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0.         0.         0.         0.        ]]\n",
            "\n",
            " [[0.76188798 0.76213958 0.76239095 0.76255841]]\n",
            "\n",
            " [[0.792209   0.8141834  0.83404912 0.84977719]]\n",
            "\n",
            " [[0.79494228 0.81839002 0.83939649 0.85584174]]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqIaVIK0_jJp"
      },
      "source": [
        "## 【問題3】（アドバンス課題）バックプロパゲーションの実装\n",
        "バックプロパゲーションを実装してください。\n",
        "\n",
        "RNNの内部は全結合層を組み合わせた形になっているので、更新式は全結合層などと同様です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCuB-VRR_Ua0"
      },
      "source": [
        "実装なし・・・"
      ]
    }
  ]
}